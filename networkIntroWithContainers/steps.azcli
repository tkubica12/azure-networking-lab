# Export variables
export rg=net-workshop

# ------------ Basic routing and VNET peering ------------
# Create Resource Group
az group create -n $rg -l centralus

# Create VNET and subnets
az network vnet create -g $rg -n hub-net --address-prefix 10.0.0.0/20

az network vnet subnet create -n jumpserver-sub --address-prefix 10.0.0.0/24 \
    -g $rg --vnet-name hub-net 
az network vnet subnet create -n sharedservices-sub --address-prefix 10.0.1.0/24\
    -g $rg --vnet-name hub-net 
az network vnet subnet create -n GatewaySubnet --address-prefix 10.0.2.0/24\
    -g $rg --vnet-name hub-net -n GatewaySubnet --address-prefix 10.0.2.0/24

# Create jump server
az vm create -n jump-vm \
    -g $rg \
    --image ubuntults \
    --size Standard_B1s \
    --admin-username demouser \
    --admin-password Azure12345678 \
    --authentication-type password \
    --nsg "" \
    --vnet-name hub-net \
    --subnet jumpserver-sub \
    --storage-sku Standard_LRS \
    --no-wait

# Create additinal server
az vm create -n hub-vm \
    -g $rg \
    --image ubuntults \
    --size Standard_B1s \
    --admin-username demouser \
    --admin-password Azure12345678 \
    --authentication-type password \
    --nsg "" \
    --vnet-name hub-net \
    --subnet sharedservices-sub \
    --storage-sku Standard_LRS \
    --no-wait

# Create gateway device (VPN GW or ER GW if you have circuit available) - to be used later
az network public-ip create -n vpn-ip -g $rg
az network vnet-gateway create -g $rg -n vpn \
    --public-ip-address vpn-ip --vnet hub-net \
    --gateway-type Vpn --sku Basic --no-wait

# Make sure you can connect to jump server
az vm list-ip-addresses -g $rg -o table
export jump=$(az vm list-ip-addresses -g $rg -n jump-vm --query [].virtualMachine.network.publicIpAddresses[].ipAddress -o tsv)
ssh demouser@$jump

# Create two spoke VNETs and subnets
az network vnet create -g $rg -n spoke1-net --address-prefix 10.0.16.0/20
az network vnet create -g $rg -n spoke2-net --address-prefix 10.0.32.0/20

az network vnet subnet create -g $rg --vnet-name spoke1-net \
    -n sub1 --address-prefix 10.0.16.0/24
az network vnet subnet create -g $rg --vnet-name spoke1-net \
    -n sub2 --address-prefix 10.0.17.0/24
az network vnet subnet create -g $rg --vnet-name spoke2-net \
    -n sub1 --address-prefix 10.0.32.0/24
az network vnet subnet create -g $rg --vnet-name spoke2-net \
    -n sub2 --address-prefix 10.0.33.0/24

# Create app server in spoke1
az vm create -n app1-vm \
    -g $rg \
    --image ubuntults \
    --size Standard_B1s \
    --admin-username demouser \
    --admin-password Azure12345678 \
    --authentication-type password \
    --nsg "" \
    --public-ip-address "" \
    --vnet-name spoke1-net \
    --subnet sub1 \
    --storage-sku Standard_LRS

# Check effective routes - jump cannot talk to app1 server, VNETs are not peered
az vm show --name app1-vm --resource-group $rg    # You can get VM details including NIC name
az network nic show-effective-route-table --name jump-vmVMNic --resource-group $rg -o table
az network nic show-effective-route-table --name jump-vmVMNic --resource-group $rg -o table

# Configure VNET peering in hub-and-spoke topology
az network vnet peering create -g $rg -n hub-to-spoke1 \
    --vnet-name hub-net --remote-vnet spoke1-net \
    --allow-vnet-access --allow-forwarded-traffic --allow-gateway-transit
az network vnet peering create -g $rg -n hub-to-spoke2 \
    --vnet-name hub-net --remote-vnet spoke2-net \
    --allow-vnet-access --allow-forwarded-traffic --allow-gateway-transit
az network vnet peering create -g $rg -n spoke1-to-hub \
    --vnet-name spoke1-net --remote-vnet hub-net \
    --allow-vnet-access --allow-forwarded-traffic --use-remote-gateways
az network vnet peering create -g $rg -n spoke2-to-hub \
    --vnet-name spoke2-net --remote-vnet hub-net \
    --allow-vnet-access --allow-forwarded-traffic --use-remote-gateways

# Check effective routes now, connect to app1 from jump server to check routing is OK
az network nic show-effective-route-table --name jump-vmVMNic --resource-group $rg -o table
ssh demouser@$jump 'ping -c 1 10.0.16.4'

# Note peering is not transitive - there is no access from spoke1 to spoke2 (we need routing device in hub for that - later in this lab)
az network nic show-effective-route-table --name app1-vmVMNic --resource-group $rg -o table



# ------------ Adding Network Security Groups ------------

# We are going to create servers in spoke2. 
# We will prepare filtering rules (NSG) on subnet to permit only web(80) inbound, but ssh(22) only from jump server
az network nsg create -g $rg -n spoke2-sub1-fw

az network nsg rule create -g $rg --nsg-name spoke2-sub1-fw \
    -n allowSSHFromJump --priority 100 \
    --source-address-prefixes 10.0.0.4/32 --source-port-ranges '*' \
    --destination-address-prefixes '*' --destination-port-ranges 22 --access Allow \
    --protocol Tcp --description "Allow SSH traffic from jump server"

az network nsg rule create -g $rg --nsg-name spoke2-sub1-fw \
    -n denySSH --priority 105 \
    --source-address-prefixes '*' --source-port-ranges '*' \
    --destination-address-prefixes '*' --destination-port-ranges 22 --access Deny \
    --protocol Tcp --description "Deny SSH traffic"

az network nsg rule create -g $rg --nsg-name spoke2-sub1-fw \
    -n allowWeb --priority 110 \
    --source-address-prefixes '*' --source-port-ranges '*' \
    --destination-address-prefixes '*' --destination-port-ranges 80 443 --access Allow \
    --protocol Tcp --description "Allow WEB traffic"

az network vnet subnet update -g $rg -n sub1 \
    --vnet-name spoke2-net --network-security-group spoke2-sub1-fw

# Create web server farm in HA deployed in two availability zones in spoke2 network
az vm create -n web1-vm \
    -g $rg \
    --image ubuntults \
    --size Standard_B1s \
    --zone 1 \
    --admin-username demouser \
    --admin-password Azure12345678 \
    --authentication-type password \
    --nsg "" \
    --public-ip-address "" \
    --vnet-name spoke2-net \
    --subnet sub1 \
    --storage-sku Standard_LRS \
    --no-wait

az vm create -n web2-vm \
    -g $rg \
    --image ubuntults \
    --size Standard_B1s \
    --zone 2 \
    --admin-username demouser \
    --admin-password Azure12345678 \
    --authentication-type password \
    --nsg "" \
    --public-ip-address "" \
    --vnet-name spoke2-net \
    --subnet sub1 \
    --storage-sku Standard_LRS \
    --no-wait

# List effective FW rules on web1 NIC
az network nic list-effective-nsg -g $rg -n web1-vmVMNic

# Check NSGs are working. SSH from jump server - should work. 
# On servers install Apache web service
az vm list-ip-addresses -g $rg -o table
ssh demouser@$jump
    ssh demouser@10.0.32.4
        sudo apt update && sudo apt install apache2 -y   # Install web server
        echo "Server 1 is alive!" | sudo tee /var/www/html/index.html  # Change default page
        curl 127.0.0.1   # Check web works
        exit
    ssh demouser@10.0.32.5
        sudo apt update && sudo apt install apache2 -y   # Install web server
        echo "Server 2 is alive!" | sudo tee /var/www/html/index.html  # Change default page
        curl 127.0.0.1   # Check web works
        exit

# SSH from jump to share vm server
# From there SSH to web1 (should fail) and run curl to web1 (should be ok)
ssh demouser@$jump
    ssh demouser@10.0.1.4
        ssh demouser@10.0.32.4  # Should fail
        curl 10.0.32.4   # Should work

# ------------ Using Azure Load Balancer ------------

# Create Azure Load Balancer Standard and configure it for internal balancer
# We will not use any session persistence so we can easily check balancing is working
## Create LB with private IP and backend pool
az network lb create -g $rg -n web-lb --sku Standard --backend-pool-name web-pool \
    --vnet-name spoke2-net --subnet sub1 --private-ip-address 10.0.32.100
## Add NICs to pool
az network nic ip-config address-pool add -g $rg \
    --nic-name web1-vmVMNic -n ipconfigweb1-vm \
    --address-pool web-pool --lb-name web-lb
az network nic ip-config address-pool add -g $rg \
    --nic-name web2-vmVMNic -n ipconfigweb2-vm \
    --address-pool web-pool --lb-name web-lb
## Configure health probe
az network lb probe create --resource-group $rg \
    --lb-name web-lb --name myHealthProbe \
    --protocol tcp --port 80
## Add LB rules
az network lb rule create --resource-group $rg \
    --lb-name web-lb --name myHTTPRule \
    --protocol tcp --frontend-port 80 --backend-port 80 \
    --frontend-ip-name LoadBalancerFrontEnd \
    --backend-pool-name web-pool \
    --probe-name myHealthProbe

# Go to jump server and test balancing
ssh demouser@$jump
    while true; do curl 10.0.32.100; done

# Check how traffic looks on web1 server (open in new session window)
# You should see health probes comming from 168.63.129.16
# Also you should see web traffic directly to client (jump server) on 10.0.0.4
ssh demouser@$jump
    ssh demouser@10.0.32.4
        sudo tcpdump -n port 80 


# ------------ Segmentation and outbound security with advanced firewall ------------

# Add subnet for firewall in hub
az network vnet subnet create -n AzureFirewallSubnet --address-prefix 10.0.3.0/24 \
    -g $rg --vnet-name hub-net 

# Create Azure Firewall - to be used later
az network public-ip create -n fw-ip -g $rg --sku Standard
az extension add -n azure-firewall -y
az network firewall policy create -n my-firewall-policy -g $rg --sku Premium
az network firewall create -n fw -g $rg --sku AZFW_VNet --tier Premium --policy my-firewall-policy
az network firewall ip-config create -f fw -g $rg -n fwipconfig --public-ip-address fw-ip --vnet-name hub-net

# Create route table to route spokes via firewall
az network route-table create -n allviafw-routes -g $rg
az network route-table route create -n all-via-firewall -g $rg --route-table-name allviafw-routes \
    --next-hop-type VirtualAppliance --address-prefix 0.0.0.0/0 --next-hop-ip-address 10.0.3.4
az network route-table route create -n hubresources-via-firewall -g $rg --route-table-name allviafw-routes \
    --next-hop-type VirtualAppliance --address-prefix 10.0.0.0/20 --next-hop-ip-address 10.0.3.4

# Create route table to route jump via firewall
az network route-table create -n intviafw-routes -g $rg
az network route-table route create -n hub-via-firewall -g $rg --route-table-name intviafw-routes \
    --next-hop-type VirtualAppliance --address-prefix 10.0.0.0/20 --next-hop-ip-address 10.0.3.4
az network route-table route create -n spoke1-via-firewall -g $rg --route-table-name intviafw-routes \
    --next-hop-type VirtualAppliance --address-prefix 10.0.16.0/20 --next-hop-ip-address 10.0.3.4
az network route-table route create -n spoke2-via-firewall -g $rg --route-table-name intviafw-routes \
    --next-hop-type VirtualAppliance --address-prefix 10.0.32.0/20 --next-hop-ip-address 10.0.3.4

# Assign routes to subnets
az network vnet subnet update -g $rg --vnet-name spoke1-net -n sub1 --route-table allviafw-routes
az network vnet subnet update -g $rg --vnet-name spoke1-net -n sub2 --route-table allviafw-routes
az network vnet subnet update -g $rg --vnet-name spoke2-net -n sub1 --route-table allviafw-routes
az network vnet subnet update -g $rg --vnet-name spoke2-net -n sub2 --route-table allviafw-routes
az network vnet subnet update -g $rg --vnet-name hub-net -n jumpserver-sub --route-table intviafw-routes
az network vnet subnet update -g $rg --vnet-name hub-net -n sharedservices-sub --route-table allviafw-routes
az network vnet subnet update -g $rg --vnet-name hub-net -n GatewaySubnet --route-table intviafw-routes

# Check routes
az network nic show-effective-route-table --name app1-vmVMNic --resource-group $rg -o table

# Firewall use implicit deny
ssh demouser@$jump
    ssh 10.0.16.4

# Allow SSH from jump to spokes
az network firewall policy rule-collection-group create -n management \
    --policy-name my-firewall-policy --priority 205 -g $rg
az network firewall policy rule-collection-group collection add-filter-collection \
    -n management-from-jump \
    --collection-priority 100 \
    --policy-name my-firewall-policy \
    --rule-collection-group-name management \
    -g $rg \
    --action Allow \
    --destination-addresses 10.0.0.0/16 \
    --destination-ports 22 3389 \
    --source-addresses 10.0.0.0/24 \
    --ip-protocols TCP \
    --rule-name SSH-RDP \
    --description SSH-RDP \
    --rule-type NetworkRule 

# Test SSH from jump
ssh demouser@$jump
    ssh 10.0.16.4  # Should work
        ssh 10.0.0.4  # Should fail (app1 is not allowed to SSH to jump)

# Allow API access from app1 in spoke1 to LB in spoke2
az network firewall policy rule-collection-group create -n apis \
    --policy-name my-firewall-policy --priority 210 -g $rg
az network firewall policy rule-collection-group collection add-filter-collection \
    -n spoke2-api \
    --collection-priority 100 \
    --policy-name my-firewall-policy \
    --rule-collection-group-name apis \
    -g $rg \
    --action Allow \
    --destination-addresses 10.0.32.100/32 \
    --destination-ports 80 \
    --source-addresses 10.0.16.0/24 \
    --ip-protocols TCP \
    --rule-name spoke1-access \
    --description spoke1-access \
    --rule-type NetworkRule 


# Test API access
ssh demouser@$jump
    curl 10.0.32.100  # Should fail (jump is not allowed to access API in spoke2)
    ssh 10.0.16.4 
        curl 10.0.32.100  # Should work

# Use application rule to allow access to specific sites
az network firewall policy rule-collection-group create -n internet-outbound \
    --policy-name my-firewall-policy --priority 220 -g $rg
az network firewall policy rule-collection-group collection add-filter-collection \
    -n remote-sites \
    --collection-priority 100 \
    --policy-name my-firewall-policy \
    --rule-collection-group-name internet-outbound \
    -g $rg \
    --action Allow \
    --target-fqdns ipconfig.io \
    --source-addresses 10.0.16.0/24 \
    --protocols Http=80 \
    --rule-name spoke1-access-ipconfigio \
    --description spoke1-access-ipconfigio \
    --rule-type ApplicationRule 

# Test internet outbound
ssh demouser@$jump
    ssh 10.0.16.4 
        curl ipconfig.io        # Should work
        curl httpbin.org/get    # Should fail

# ------------ Kubernetes networking ------------

# Configure outbound rules for AKS (registries etc.)
az network firewall policy rule-collection-group collection add-filter-collection \
    -n aks \
    --collection-priority 105 \
    --policy-name my-firewall-policy \
    --rule-collection-group-name internet-outbound \
    -g $rg \
    --action Allow \
    --fqdn-tags AzureKubernetesService \
    --source-addresses 10.0.0.0/16 \
    --protocols Http=80 Https=443 \
    --rule-name aks \
    --description aks \
    --rule-type ApplicationRule 
az network firewall policy rule-collection-group collection rule add \
    --collection-name  aks \
    --policy-name my-firewall-policy \
    --rule-collection-group-name internet-outbound \
    -g $rg \
    --target-fqdns '*.docker.io' '*.docker.com' \
    --source-addresses 10.0.0.0/16 \
    --protocols Https=443 \
    -n docker-hub \
    --description docker-hub \
    --rule-type ApplicationRule 

# Create Azure Kubernetes Service in private networking mode
az aks create \
    --resource-group $rg \
    --name myaks \
    --load-balancer-sku standard \
    --enable-private-cluster \
    --network-plugin azure \
    --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name spoke1-net -n sub2 --query id -o tsv) \
    --docker-bridge-address 172.17.0.1/16 \
    --dns-service-ip 10.2.0.10 \
    --service-cidr 10.2.0.0/24 \
    --network-policy azure

# Map private DNS to hub network
export nodeResourceGroup=$(az aks show -n myaks -g $rg --query nodeResourceGroup -o tsv)
export zone=$(az aks show -n myaks -g $rg --query privateFqdn -o tsv | awk -F '.' '{ print $2"."$3"."$4"."$5"."$6 }')
az network private-dns link vnet create -n link-hub --registration-enabled false -g $nodeResourceGroup \
        --virtual-network $(az network vnet show -n hub-net -g $rg --query id -o tsv) \
        --zone-name $zone

# Connect to jump server, install Azure CLI, Kubernetes tools and connect to cluster
ssh demouser@$jump
    curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
    az login
    az account set -s tokubica
    az aks get-credentials -n myaks -g net-workshop --admin
    sudo az aks install-cli
    wget https://github.com/derailed/k9s/releases/download/v0.24.15/k9s_Linux_x86_64.tar.gz
    tar xvf k9s_Linux_x86_64.tar.gz
    sudo mv k9s /usr/bin
    kubectl get nodes   # CLI tool
    k9s  # type :nodes to get nodes - ASCII based GUI
    # Create application deployment
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myweb-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
      - name: myweb
        image: tkubica/web:python-1
        env:
        - name: PORT
          value: "80"
        - name: INFO
          value: "aks"
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 64M
          limits:
            cpu: 500m
            memory: 256M
EOF
    # Create service and exposed using Azure LB on private IP
    cat <<EOF | kubectl apply -f -
kind: Service
apiVersion: v1
metadata:
  name: myweb-service-ext-private-static
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"
spec:
  loadBalancerIP: 10.0.17.100
  selector:
    app: myweb
  type: LoadBalancer
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
EOF

# Test service from app1
ssh demouser@$jump
    ssh 10.0.16.4
    curl 10.0.17.100

# Using network policy
ssh demouser@$jump
    # Create apps and intruder
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
    name: app1
    labels:
        zone: app1
---
apiVersion: v1
kind: Namespace
metadata:
    name: app2
    labels:
        zone: app2
---
apiVersion: v1
kind: Namespace
metadata:
    name: intruder
    labels:
        zone: intruder
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
  namespace: app1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app2
  namespace: app2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app2
  template:
    metadata:
      labels:
        app: app2
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: intruder
  namespace: intruder
spec:
  replicas: 2
  selector:
    matchLabels:
      app: intruder
  template:
    metadata:
      labels:
        app: intruder
    spec:
      containers:
      - name: nginx
        image: nginx
---
kind: Service
apiVersion: v1
metadata:
  name: app2
  namespace: app2
spec:
  selector:
    app: app2
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
EOF
    # Use k9s to exec to intruder container and access app2 (curl app2.app2.svc.cluster.local)
    # This works - all pods can access each other

    # Create Network Policy so app2 can be accessed only from namespace app1
    cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app2-network-policy
  namespace: app2
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          zone: app1
    ports:
    - protocol: TCP
      port: 80
EOF
        
    # Use k9s to exec to intruder container and access app2 (curl app2.app2.svc.cluster.local) - should fail
    # Use k9s to exec to app1 container and access app2 (curl app2.app2.svc.cluster.local) - should work
